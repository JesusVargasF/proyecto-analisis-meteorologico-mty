# **ANLISIS Y MODELO DE MACHINE LEARNING PARA DATOS METEOROLGICOS DE MONTERREY, NUEVO LEN, MXICO**

##  Objetivo del Proyecto

El prop贸sito de este proyecto fue obtener y analizar datos meteorol贸gicos utilizando la **API de Open-Meteor**, aplicar un proceso ETL en **AWS** y desarrollar un modelo de aprendizaje autom谩tico en **Google Colab** para evaluar patrones clim谩ticos. En este proyecto, utilic茅 herramientas de **AWS** y **Python** para procesar informaci贸n clim谩tica y generar predicciones 煤tiles.

---

### 1锔 Obtenci贸n de Datos

- Utilic茅 la API de Open-Meteor para extraer informaci贸n meteorol贸gica diaria. Los par谩metros clave obtenidos fueron:
  - Temperatura m谩xima (`temperature_2m_max`)
  - Temperatura m铆nima (`temperature_2m_min`)
  - Temperatura media (`temperature_2m_mean`)
  - Precipitaci贸n total (`rain_sum`)
  - Horas de precipitaci贸n (`precipitation_hours`)
  - Velocidad m谩xima del viento a 10m (`wind_speed_10m_max`)
  - Radiaci贸n solar acumulada (`shortwave_radiation_sum`)

- **Rango de Fechas**: 01 de Enero de 2020 a 31 de diciembre de 2024

- **Validaci贸n de Datos**:
  - Realic茅 una revision para detectar campos duplicados.
  - Verifiqu茅 que no hubiera valores nulos.

---

### 2锔 Proceso ETL en AWS

Implement茅 un flujo de datos en AWS para procesar y almacenar los datos. A continuaci贸n, describo los pasos que segu铆:

- **Extracci贸n de Datos con AWS Lambda**:
  - Utilic茅 **AWS Lambda** para conectarme a la API de Open-Meteor y extraer los datos meteorol贸gicos diarios de las fechas establecidas y almacenar los datos crudos en un bucket de **Amazon S3**.

- **Almacenamiento de Datos en Amazon S3**:
  - Los datos originales obtenidos de la API se almacenaron en un bucket de **Amazon S3**. Este bucket sirvi贸 como almacenamiento para los datos en su estado original y ya procesados.

Bucket en la region us-east-1:

![image](https://github.com/user-attachments/assets/87ff4ac5-1d3f-4346-bf4b-4531596ee9e3)

Datos originales:

![image](https://github.com/user-attachments/assets/095c2823-cc5d-4c1c-83e9-b86e3b8800df)

Datos procesados:

![image](https://github.com/user-attachments/assets/4013b13e-f415-495d-b44c-17d25361d632)


- **Catalogaci贸n de Datos con AWS Glue Crawler**:
  - Us茅 **AWS Glue Crawler** para escanear los datos crudos almacenados en S3 y generar un esquema estructurado en **AWS Glue Data Catalog**. Esto permiti贸 organizar los datos de manera que pudieran ser consultados f谩cilmente en futuros an谩lisis.

![image](https://github.com/user-attachments/assets/59662723-52b0-49c8-ad7f-786256d0f787)


- **Transformaci贸n de Datos con AWS Glue Job**:
  - Implement茅 un **AWS Glue Job** para realizar las siguientes transformaciones en los datos:
    - Cambiar el tipo de dato de la columna de fecha a `DATE`.
    - Convertir las columnas num茅ricas (como temperatura, precipitaci贸n, etc.) a tipos de datos adecuados (`FLOAT`).
    - Agregar un campo `id` 煤nico para cada registro, facilitando la identificaci贸n.

Visual ETL Job:

![image](https://github.com/user-attachments/assets/8f92135f-94fb-4b4c-a353-70efb4d71d3f)
